{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulinaWalasiewicz/Colab-notebooks/blob/main/Copy_of_NN_model_training_with_tabular_data_Vistula.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import niezbednych bibliotek\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler"
      ],
      "metadata": {
        "id": "Z2oVN4obSup7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przygotowanie przykładowych danych\n",
        "data = {\n",
        "    'Category': ['Electronics', 'Furniture', 'Clothing', 'Electronics', 'Clothing'],\n",
        "    'Region': ['North', 'South', 'East', 'West', 'North'],\n",
        "    'Sales': [200, 300, 150, 400, 250],\n",
        "    'Time on Platform': [30, 45, 20, 50, 35],\n",
        "    'Target': [1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "# Konwersja danych do pandas DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "EAJWdXsHS-J4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funkcja do kodowania wartości kategorycznych\n",
        "def encode_categorical_columns(df, columns):\n",
        "    label_encoders = {}\n",
        "    for col in columns:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "    return label_encoders\n",
        "\n",
        "# Funkcja do standaryzacji wartości numerycznych\n",
        "def standardize_columns(df, columns):\n",
        "    scaler = StandardScaler()\n",
        "    df[columns] = scaler.fit_transform(df[columns])\n",
        "    return scaler\n",
        "\n",
        "# Kodowanie kolumn kategorycznych\n",
        "categorical_columns = ['Category', 'Region']\n",
        "label_encoders = encode_categorical_columns(df, categorical_columns)\n",
        "\n",
        "# Standaryzacja kolumn numerycznych\n",
        "numerical_columns = ['Sales', 'Time on Platform']\n",
        "scaler = standardize_columns(df, numerical_columns)\n",
        "\n",
        "# Podział danych na zbiory treningowy i testowy\n",
        "features = ['Category', 'Region', 'Sales', 'Time on Platform']\n",
        "X = df[features]\n",
        "y = df['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "mgXF4ryzTK6Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definicja modelu embeddingów\n",
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, embedding_sizes, num_numerical_features):\n",
        "        \"\"\"\n",
        "        Inicjalizacja modelu.\n",
        "        embedding_sizes: lista krotek (liczba kategorii, rozmiar embeddingow) dla zmiennych kategorycznych.\n",
        "        num_numerical_features: liczba cech numerycznych.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Tworzenie listy warstw embeddingow dla zmiennych kategorycznych\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(num_embeddings=categories, embedding_dim=size)\n",
        "            for categories, size in embedding_sizes\n",
        "        ])\n",
        "\n",
        "        # Warstwy w pełni połączone (fully connected)\n",
        "        input_size = sum([size for _, size in embedding_sizes]) + num_numerical_features\n",
        "        self.fc1 = nn.Linear(input_size, 32)  # Pierwsza warstwa w pełni połączona\n",
        "        self.fc2 = nn.Linear(32, 32)         # Druga warstwa w pełni połączona\n",
        "        self.fc3 = nn.Linear(32, 16)         # Druga warstwa w pełni połączona\n",
        "        self.output = nn.Linear(16, 1)      # Warstwa wyjściowa\n",
        "        self.dropout = nn.Dropout(0.2)      # Dropout dla regularyzacji\n",
        "\n",
        "    def forward(self, x_categorical, x_numerical):\n",
        "        \"\"\"\n",
        "        Propagacja w przód (forward pass).\n",
        "        x_categorical: tensor wejściowy dla zmiennych kategorycznych.\n",
        "        x_numerical: tensor wejściowy dla cech numerycznych.\n",
        "        \"\"\"\n",
        "        # Embeddingi dla zmiennych kategorycznych\n",
        "        embedded = [emb(x_categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
        "        embedded = torch.cat(embedded, dim=1)  # Konkatenacja embeddingow\n",
        "\n",
        "        # Połączenie embeddingow ze zmiennymi numerycznymi\n",
        "        x = torch.cat([embedded, x_numerical], dim=1)\n",
        "\n",
        "        # Przepływ przez warstwy w pełni połączone z funkcją aktywacji ReLU\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(torch.relu(self.fc3(x)))\n",
        "\n",
        "        # Warstwa wyjściowa z funkcją aktywacji sigmoid\n",
        "        logits = torch.sigmoid(self.output(x))\n",
        "        return logits,x"
      ],
      "metadata": {
        "id": "i5DF4HgGTdld"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZTm6-i3gQ4sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca16d630-c3a1-43fa-d846-188c109d2b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmbeddingModel(\n",
            "  (embeddings): ModuleList(\n",
            "    (0): Embedding(3, 4)\n",
            "    (1): Embedding(4, 4)\n",
            "  )\n",
            "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Określenie rozmiarów wektorów embeddingów dla zmiennych kategorycznych\n",
        "# embedding_sizes: lista krotek (liczba unikalnych wartości w kolumnie, rozmiar embeddingu)\n",
        "embedding_sizes = [\n",
        "    (df['Category'].nunique(), 4),  # 'Product Category': liczba kategorii i rozmiar embeddingu\n",
        "    (df['Region'].nunique(), 4)   # 'Customer Region': liczba regionów i rozmiar embeddingu\n",
        "]\n",
        "\n",
        "# Konwersja danych na tensory\n",
        "# Dane kategoryczne (train)\n",
        "X_categorical_train = torch.tensor(\n",
        "    X_train[['Category', 'Region']].values, dtype=torch.long\n",
        ")\n",
        "\n",
        "# Dane numeryczne (train)\n",
        "X_numerical_train = torch.tensor(\n",
        "    X_train[['Sales', 'Time on Platform']].values, dtype=torch.float\n",
        ")\n",
        "\n",
        "# Target (wartości wyjściowe) jako tensor (przekształcenie na kolumnowy tensor)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "# Utworzenie instancji modelu oraz optymalizatora gradientowego\n",
        "# EmbeddingModel: model uwzględniający embeddingi i dane numeryczne\n",
        "model = EmbeddingModel(embedding_sizes, num_numerical_features=2)\n",
        "\n",
        "# Kryterium strat: funkcja Binary Cross-Entropy (BCELoss)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optymalizator: Adam z ustaloną wartością learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Pętla trenująca model\n",
        "epochs = 50  # Liczba epok\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()                  # Zerowanie gradientów\n",
        "    outputs,_ = model(X_categorical_train, X_numerical_train)  # Przepływ danych przez model\n",
        "    loss = criterion(outputs, y_train_tensor)  # Obliczanie strat\n",
        "    loss.backward()                     # Wyznaczanie gradientów\n",
        "    optimizer.step()                    # Aktualizacja wag modelu\n",
        "\n",
        "# Wyświetlenie oceny modelu w trybie ewaluacji\n",
        "print(model.eval())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/model/trained_model_2.pth\"\n",
        "# path = \"/content/trained_model.pth\"\n",
        "torch.save(model, path)"
      ],
      "metadata": {
        "id": "1s0AYLp_voE9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(path)\n",
        "checkpoint.eval()"
      ],
      "metadata": {
        "id": "oql_Dzl3wEXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4107ca1-b8c9-4fca-936f-dfe89f20d2a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-518e4102219a>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EmbeddingModel(\n",
              "  (embeddings): ModuleList(\n",
              "    (0): Embedding(3, 4)\n",
              "    (1): Embedding(4, 4)\n",
              "  )\n",
              "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Przygotowanie danych do inferencji modelu\n",
        "# ZAD1. Wykonajcie inferencje i zapiszcie wynik dla 10 roznych wartosci data_inf\n",
        "data_inf = {\n",
        "    'Category': ['Electronics', 'Furniture', 'Clothing', 'Electronics', 'Clothing', 'Electronics', 'Furniture', 'Clothing', 'Electronics', 'Clothing'],\n",
        "    'Region': ['North', 'South', 'East', 'West', 'North', 'North', 'South', 'East', 'West', 'North'],\n",
        "    'Sales': [200, 300, 150, 400, 250, 200, 300, 150, 400, 250],\n",
        "    'Time on Platform': [30, 45, 20, 50, 35, 30, 45, 20, 50, 35]\n",
        "}\n",
        "\n",
        "# Konwersja do pandas DataFrame\n",
        "df_inf = pd.DataFrame(data_inf)\n",
        "\n",
        "# Kodowanie kolumn kategorycznych\n",
        "label_encoders = encode_categorical_columns(df_inf, categorical_columns)\n",
        "\n",
        "# Standaryzacja kolumn numerycznych\n",
        "scaler = standardize_columns(df_inf, numerical_columns)\n",
        "\n",
        "# Podział danych na zbiory treningowy i testowy\n",
        "features = ['Category', 'Region', 'Sales', 'Time on Platform']\n",
        "X = df_inf[features]\n",
        "\n",
        "X_categorical_inf = torch.tensor(\n",
        "    X[['Category', 'Region']].values, dtype=torch.long\n",
        ")\n",
        "\n",
        "# Dane numeryczne (train)\n",
        "X_numerical_inf = torch.tensor(\n",
        "    X[['Sales', 'Time on Platform']].values, dtype=torch.float\n",
        ")\n",
        "\n",
        "output_inf = checkpoint(X_categorical_inf, X_numerical_inf)\n",
        "\n",
        "# Wynik inferencji to tzw. logit, im blizej, 0 tym wieksza pewnosc modelu ze Target = 0, im blizej 1, wym wieksza pewnosc modelu ze target = 1. Wszystko pomiedzy 0 i 1 obarczone jest niepewnoscia modelu co do poprawnego wyniku, przy czym najwieksza niepewnosc modelu oznacza wynik inferencji - 0.5\n",
        "print(output_inf)"
      ],
      "metadata": {
        "id": "OZcbeaYkllFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2c4163-d036-4bd9-a46d-0bae8d7a4600"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[1.0000e+00],\n",
            "        [3.3526e-03],\n",
            "        [1.0000e+00],\n",
            "        [3.9072e-11],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [3.3526e-03],\n",
            "        [1.0000e+00],\n",
            "        [3.9072e-11],\n",
            "        [1.0000e+00]], grad_fn=<SigmoidBackward0>), tensor([[29.7647,  0.0000,  0.0000, 26.1190,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 22.9571,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0533,  3.7829,  0.0000,  0.2429,  3.5930,  0.0000,  0.0000,  3.4123,\n",
            "          0.0000,  0.0000,  3.6712,  0.0000,  0.0000,  0.0000,  3.9063,  1.9202],\n",
            "        [28.7151,  0.0000,  0.0000, 24.8327,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 21.9016,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, 15.2049,  0.0000,  0.0000, 15.0602,  0.0000,  0.0000, 13.8681,\n",
            "          0.0000,  0.0000, 15.3042,  0.0000,  0.0000,  0.0000, 16.4147,  8.0222],\n",
            "        [30.7005,  0.0000,  0.0000, 26.7840,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 23.5949,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [29.7647,  0.0000,  0.0000, 26.1190,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 22.9571,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0533,  3.7829,  0.0000,  0.2429,  3.5930,  0.0000,  0.0000,  3.4123,\n",
            "          0.0000,  0.0000,  3.6712,  0.0000,  0.0000,  0.0000,  3.9063,  1.9202],\n",
            "        [28.7151,  0.0000,  0.0000, 24.8327,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 21.9016,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, 15.2049,  0.0000,  0.0000, 15.0602,  0.0000,  0.0000, 13.8681,\n",
            "          0.0000,  0.0000, 15.3042,  0.0000,  0.0000,  0.0000, 16.4147,  8.0222],\n",
            "        [30.7005,  0.0000,  0.0000, 26.7840,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 23.5949,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "       grad_fn=<ReluBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ZAD2. Zmodyfikujcie kod tak, aby oprocz wyniku (logit) zostal rowniez zwracany embedding dla rekordu na wyjciu warstwy fc2. Wskazowka: Zmian nalezy dokonac tylko w czesci # Zdefiniowanie modelu\n",
        "# ZAD3. W definicji modelu, dodajcie kolejna, trzecia warstwe liniowa w pelni polaczona fc3, tak aby fc2 miala wymiar (32, 32) oraz fc3 miala wymiar (32,16)\n",
        "# ZAD4. Poeksperymentujcie jak wplynie na dokladnosc przewidywania modelu zmiana nastepujacych parametrow modelu i trenera:\n",
        "#   - dodanie kolejnych warstw liniowych do modelu fc4 i fc5\n",
        "#   - zmiana wymiaru wejsie-wyjscie warstw liniowych fc1, fc2, fc3, ...\n",
        "#   - zmniejszenie lub zwiększenie (max lr = 1) wartosci kroku w optymalizatorze (lr)"
      ],
      "metadata": {
        "id": "eLQUWPpXeEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedEmbeddingModel(nn.Module):\n",
        "    def __init__(self, embedding_sizes, num_numerical_features):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(num_embeddings=categories, embedding_dim=size)\n",
        "            for categories, size in embedding_sizes\n",
        "        ])\n",
        "        input_size = sum([size for _, size in embedding_sizes]) + num_numerical_features\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 8)  # New Layer\n",
        "        self.fc5 = nn.Linear(8, 4)  # New Layer\n",
        "        self.output = nn.Linear(4, 1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x_categorical, x_numerical):\n",
        "        embedded = [emb(x_categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
        "        embedded = torch.cat(embedded, dim=1)\n",
        "        x = torch.cat([embedded, x_numerical], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))  # Pass through fc4\n",
        "        x = torch.relu(self.fc5(x))  # Pass through fc5\n",
        "        logits = torch.sigmoid(self.output(x))\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "akVZgpIVJe2R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ExtendedEmbeddingModel(embedding_sizes, num_numerical_features=2)\n",
        "\n",
        "# Kryterium strat: funkcja Binary Cross-Entropy (BCELoss)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optymalizator: Adam z ustaloną wartością learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Pętla trenująca model\n",
        "epochs = 50  # Liczba epok\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()                  # Zerowanie gradientów\n",
        "    outputs= model(X_categorical_train, X_numerical_train)  # Przepływ danych przez model\n",
        "    loss = criterion(outputs, y_train_tensor)  # Obliczanie strat\n",
        "    loss.backward()                     # Wyznaczanie gradientów\n",
        "    optimizer.step()                    # Aktualizacja wag modelu\n",
        "\n",
        "# Wyświetlenie oceny modelu w trybie ewaluacji\n",
        "print(model.eval())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfWX_RcAKFdG",
        "outputId": "cfdac1a4-4b8e-41a1-8d10-a465547c541e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExtendedEmbeddingModel(\n",
            "  (embeddings): ModuleList(\n",
            "    (0): Embedding(3, 4)\n",
            "    (1): Embedding(4, 4)\n",
            "  )\n",
            "  (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (fc5): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/model/trained_model_3.pth\"\n",
        "# path = \"/content/trained_model.pth\"\n",
        "torch.save(model, path)"
      ],
      "metadata": {
        "id": "XYtCRA7SKaYv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint2 = torch.load(path)\n",
        "checkpoint2.eval()\n",
        "output_inf = checkpoint2(X_categorical_inf, X_numerical_inf)\n",
        "\n",
        "# Wynik inferencji to tzw. logit, im blizej, 0 tym wieksza pewnosc modelu ze Target = 0, im blizej 1, wym wieksza pewnosc modelu ze target = 1. Wszystko pomiedzy 0 i 1 obarczone jest niepewnoscia modelu co do poprawnego wyniku, przy czym najwieksza niepewnosc modelu oznacza wynik inferencji - 0.5\n",
        "print(output_inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EodACAsOPemE",
        "outputId": "0cd8a896-df09-4101-a08c-21a1389b92b1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00],\n",
            "        [5.4603e-01],\n",
            "        [1.0000e+00],\n",
            "        [5.8786e-18],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [5.4603e-01],\n",
            "        [1.0000e+00],\n",
            "        [5.8786e-18],\n",
            "        [1.0000e+00]], grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-d051542a283c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint2 = torch.load(path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedDimensionsEmbeddingModel(nn.Module):\n",
        "    def __init__(self, embedding_sizes, num_numerical_features):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(num_embeddings=categories, embedding_dim=size)\n",
        "            for categories, size in embedding_sizes\n",
        "        ])\n",
        "        input_size = sum([size for _, size in embedding_sizes]) + num_numerical_features\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Increased size\n",
        "        self.fc2 = nn.Linear(128, 64)         # Increased size\n",
        "        self.fc3 = nn.Linear(64, 32)          # Increased size\n",
        "        self.output = nn.Linear(32, 1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x_categorical, x_numerical):\n",
        "        embedded = [emb(x_categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
        "        embedded = torch.cat(embedded, dim=1)\n",
        "        x = torch.cat([embedded, x_numerical], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(torch.relu(self.fc3(x)))\n",
        "        logits = torch.sigmoid(self.output(x))\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "tNWBaF9zJqL5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModifiedDimensionsEmbeddingModel(embedding_sizes, num_numerical_features=2)\n",
        "\n",
        "# Kryterium strat: funkcja Binary Cross-Entropy (BCELoss)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optymalizator: Adam z ustaloną wartością learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Pętla trenująca model\n",
        "epochs = 50  # Liczba epok\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()                  # Zerowanie gradientów\n",
        "    outputs= model(X_categorical_train, X_numerical_train)  # Przepływ danych przez model\n",
        "    loss = criterion(outputs, y_train_tensor)  # Obliczanie strat\n",
        "    loss.backward()                     # Wyznaczanie gradientów\n",
        "    optimizer.step()                    # Aktualizacja wag modelu\n",
        "\n",
        "# Wyświetlenie oceny modelu w trybie ewaluacji\n",
        "print(model.eval())\n",
        "\n",
        "path = \"/content/model/trained_model_4.pth\"\n",
        "# path = \"/content/trained_model.pth\"\n",
        "torch.save(model, path)\n",
        "\n",
        "checkpoint3 = torch.load(path)\n",
        "checkpoint3.eval()\n",
        "output_inf = checkpoint3(X_categorical_inf, X_numerical_inf)\n",
        "\n",
        "# Wynik inferencji to tzw. logit, im blizej, 0 tym wieksza pewnosc modelu ze Target = 0, im blizej 1, wym wieksza pewnosc modelu ze target = 1. Wszystko pomiedzy 0 i 1 obarczone jest niepewnoscia modelu co do poprawnego wyniku, przy czym najwieksza niepewnosc modelu oznacza wynik inferencji - 0.5\n",
        "print(output_inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J1ivKdRJurh",
        "outputId": "661a0fc8-dca4-4645-abc0-81891aac902a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModifiedDimensionsEmbeddingModel(\n",
            "  (embeddings): ModuleList(\n",
            "    (0): Embedding(3, 4)\n",
            "    (1): Embedding(4, 4)\n",
            "  )\n",
            "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n",
            "tensor([[1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [1.0701e-19],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [1.0701e-19],\n",
            "        [1.0000e+00]], grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-81fbd2ce83ee>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint3 = torch.load(path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the default EmbeddingModel\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]  # Varying learning rates\n",
        "\n",
        "results = {}\n",
        "for lr in learning_rates:\n",
        "    model = EmbeddingModel(embedding_sizes, num_numerical_features=2)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(X_categorical_train, X_numerical_train)  # Unpack the tuple\n",
        "        loss = criterion(logits, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate performance and store results\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred, _ = model(X_categorical_train, X_numerical_train)  # Unpack here as well\n",
        "        y_pred_binary = (y_pred > 0.5).float()\n",
        "        accuracy = (y_pred_binary == y_train_tensor).float().mean().item()\n",
        "        results[lr] = accuracy\n",
        "\n",
        "# Print results\n",
        "print(\"Learning Rate Results:\")\n",
        "for lr, acc in results.items():\n",
        "    print(f\"Learning Rate: {lr}, Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt8RiEO_Rc-Y",
        "outputId": "11062d11-7963-49ba-b72c-e027334f4484"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate Results:\n",
            "Learning Rate: 0.001, Accuracy: 1.0000\n",
            "Learning Rate: 0.01, Accuracy: 1.0000\n",
            "Learning Rate: 0.1, Accuracy: 1.0000\n",
            "Learning Rate: 0.5, Accuracy: 0.7500\n",
            "Learning Rate: 1.0, Accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RsrniAKySIcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}